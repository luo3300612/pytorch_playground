2019-06-05 10:52:50,572 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.01, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 10:52:54,960 - INFO: Iter: 10/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:52:55,307 - INFO: Iter: 20/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:52:55,647 - INFO: Iter: 30/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:52:55,987 - INFO: Iter: 40/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:52:56,345 - INFO: Iter: 50/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:52:56,696 - INFO: Iter: 60/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:52:57,025 - INFO: Iter: 70/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:52:57,371 - INFO: Iter: 80/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:52:57,732 - INFO: Iter: 90/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:52:58,075 - INFO: Iter: 100/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:52:58,488 - INFO: Iter: 110/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:52:58,858 - INFO: Iter: 120/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:52:59,469 - INFO: Iter: 130/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:53:00,335 - INFO: Iter: 140/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:53:00,922 - INFO: Iter: 150/64000	Loss:2.302583	LR: 0.01
2019-06-05 10:53:13,964 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.01, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 10:53:15,786 - INFO: Iter: 10/64000	Loss:2.309413	LR: 0.01
2019-06-05 10:53:16,013 - INFO: Iter: 20/64000	Loss:2.298335	LR: 0.01
2019-06-05 10:53:16,251 - INFO: Iter: 30/64000	Loss:2.302320	LR: 0.01
2019-06-05 10:53:16,498 - INFO: Iter: 40/64000	Loss:2.293401	LR: 0.01
2019-06-05 10:53:16,743 - INFO: Iter: 50/64000	Loss:2.279479	LR: 0.01
2019-06-05 10:53:16,980 - INFO: Iter: 60/64000	Loss:2.265502	LR: 0.01
2019-06-05 10:53:17,234 - INFO: Iter: 70/64000	Loss:2.240631	LR: 0.01
2019-06-05 10:53:17,473 - INFO: Iter: 80/64000	Loss:2.194775	LR: 0.01
2019-06-05 10:53:17,708 - INFO: Iter: 90/64000	Loss:2.231459	LR: 0.01
2019-06-05 10:53:17,950 - INFO: Iter: 100/64000	Loss:2.096006	LR: 0.01
2019-06-05 10:53:18,186 - INFO: Iter: 110/64000	Loss:2.098829	LR: 0.01
2019-06-05 10:53:18,422 - INFO: Iter: 120/64000	Loss:2.057347	LR: 0.01
2019-06-05 10:53:18,669 - INFO: Iter: 130/64000	Loss:1.949813	LR: 0.01
2019-06-05 10:53:18,914 - INFO: Iter: 140/64000	Loss:1.913523	LR: 0.01
2019-06-05 10:53:19,163 - INFO: Iter: 150/64000	Loss:1.948547	LR: 0.01
2019-06-05 10:53:19,414 - INFO: Iter: 160/64000	Loss:1.959300	LR: 0.01
2019-06-05 10:53:19,664 - INFO: Iter: 170/64000	Loss:1.898891	LR: 0.01
2019-06-05 10:53:19,903 - INFO: Iter: 180/64000	Loss:1.918034	LR: 0.01
2019-06-05 10:53:20,140 - INFO: Iter: 190/64000	Loss:1.958091	LR: 0.01
2019-06-05 10:53:20,398 - INFO: Iter: 200/64000	Loss:1.877589	LR: 0.01
2019-06-05 10:53:20,645 - INFO: Iter: 210/64000	Loss:1.734807	LR: 0.01
2019-06-05 10:53:20,893 - INFO: Iter: 220/64000	Loss:1.817718	LR: 0.01
2019-06-05 10:53:21,114 - INFO: Iter: 230/64000	Loss:1.798074	LR: 0.01
2019-06-05 10:53:21,362 - INFO: Iter: 240/64000	Loss:1.749856	LR: 0.01
2019-06-05 10:53:21,598 - INFO: Iter: 250/64000	Loss:1.827799	LR: 0.01
2019-06-05 10:53:21,847 - INFO: Iter: 260/64000	Loss:1.859612	LR: 0.01
2019-06-05 10:53:22,087 - INFO: Iter: 270/64000	Loss:1.658121	LR: 0.01
2019-06-05 10:53:22,329 - INFO: Iter: 280/64000	Loss:1.851343	LR: 0.01
2019-06-05 10:53:22,572 - INFO: Iter: 290/64000	Loss:1.894979	LR: 0.01
2019-06-05 10:53:22,814 - INFO: Iter: 300/64000	Loss:1.771654	LR: 0.01
2019-06-05 10:53:23,059 - INFO: Iter: 310/64000	Loss:1.749161	LR: 0.01
2019-06-05 10:53:23,299 - INFO: Iter: 320/64000	Loss:1.713847	LR: 0.01
2019-06-05 10:53:23,530 - INFO: Iter: 330/64000	Loss:1.619852	LR: 0.01
2019-06-05 10:53:23,777 - INFO: Iter: 340/64000	Loss:1.685581	LR: 0.01
2019-06-05 10:53:24,014 - INFO: Iter: 350/64000	Loss:1.720114	LR: 0.01
2019-06-05 10:53:24,253 - INFO: Iter: 360/64000	Loss:1.782406	LR: 0.01
2019-06-05 10:53:24,496 - INFO: Iter: 370/64000	Loss:1.644124	LR: 0.01
2019-06-05 10:53:24,719 - INFO: Iter: 380/64000	Loss:1.738985	LR: 0.01
2019-06-05 10:53:24,907 - INFO: Iter: 390/64000	Loss:1.577529	LR: 0.01
2019-06-05 10:53:25,240 - INFO: Iter: 400/64000	Loss:1.681061	LR: 0.01
2019-06-05 10:53:25,491 - INFO: Iter: 410/64000	Loss:1.678306	LR: 0.01
2019-06-05 10:53:25,746 - INFO: Iter: 420/64000	Loss:1.645231	LR: 0.01
2019-06-05 10:53:25,994 - INFO: Iter: 430/64000	Loss:1.620834	LR: 0.01
2019-06-05 10:53:26,230 - INFO: Iter: 440/64000	Loss:1.643318	LR: 0.01
2019-06-05 10:53:26,474 - INFO: Iter: 450/64000	Loss:1.595290	LR: 0.01
2019-06-05 10:53:26,719 - INFO: Iter: 460/64000	Loss:1.711482	LR: 0.01
2019-06-05 10:53:26,975 - INFO: Iter: 470/64000	Loss:1.713054	LR: 0.01
2019-06-05 10:53:27,214 - INFO: Iter: 480/64000	Loss:1.806830	LR: 0.01
2019-06-05 10:53:27,470 - INFO: Iter: 490/64000	Loss:1.681224	LR: 0.01
2019-06-05 10:53:27,706 - INFO: Iter: 500/64000	Loss:1.717174	LR: 0.01
2019-06-05 10:53:27,959 - INFO: Iter: 510/64000	Loss:1.649560	LR: 0.01
2019-06-05 10:53:28,198 - INFO: Iter: 520/64000	Loss:1.495206	LR: 0.01
2019-06-05 10:53:28,427 - INFO: Iter: 530/64000	Loss:1.665978	LR: 0.01
2019-06-05 10:53:28,672 - INFO: Iter: 540/64000	Loss:1.627011	LR: 0.01
2019-06-05 10:53:28,934 - INFO: Iter: 550/64000	Loss:1.603697	LR: 0.01
2019-06-05 10:53:29,168 - INFO: Iter: 560/64000	Loss:1.553698	LR: 0.01
2019-06-05 10:53:29,421 - INFO: Iter: 570/64000	Loss:1.447508	LR: 0.01
2019-06-05 10:53:29,675 - INFO: Iter: 580/64000	Loss:1.580877	LR: 0.01
2019-06-05 10:53:29,917 - INFO: Iter: 590/64000	Loss:1.612986	LR: 0.01
2019-06-05 10:53:30,156 - INFO: Iter: 600/64000	Loss:1.648944	LR: 0.01
2019-06-05 10:53:30,393 - INFO: Iter: 610/64000	Loss:1.574005	LR: 0.01
2019-06-05 11:00:41,640 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.01, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 11:00:43,995 - INFO: Iter: 10/64000	Loss:2.304320	LR: 0.01
2019-06-05 11:00:44,755 - INFO: Iter: 20/64000	Loss:2.301539	LR: 0.01
2019-06-05 11:00:45,494 - INFO: Iter: 30/64000	Loss:2.300884	LR: 0.01
2019-06-05 11:00:46,255 - INFO: Iter: 40/64000	Loss:2.304787	LR: 0.01
2019-06-05 11:00:46,989 - INFO: Iter: 50/64000	Loss:2.305853	LR: 0.01
2019-06-05 11:00:47,733 - INFO: Iter: 60/64000	Loss:2.302325	LR: 0.01
2019-06-05 11:00:48,482 - INFO: Iter: 70/64000	Loss:2.307932	LR: 0.01
2019-06-05 11:00:49,248 - INFO: Iter: 80/64000	Loss:2.304047	LR: 0.01
2019-06-05 11:00:49,988 - INFO: Iter: 90/64000	Loss:2.306949	LR: 0.01
2019-06-05 11:00:50,722 - INFO: Iter: 100/64000	Loss:2.300529	LR: 0.01
2019-06-05 11:00:51,453 - INFO: Iter: 110/64000	Loss:2.302297	LR: 0.01
2019-06-05 11:00:52,189 - INFO: Iter: 120/64000	Loss:2.299997	LR: 0.01
2019-06-05 11:00:52,936 - INFO: Iter: 130/64000	Loss:2.303218	LR: 0.01
2019-06-05 11:00:53,682 - INFO: Iter: 140/64000	Loss:2.303708	LR: 0.01
2019-06-05 11:00:54,427 - INFO: Iter: 150/64000	Loss:2.304862	LR: 0.01
2019-06-05 11:00:55,163 - INFO: Iter: 160/64000	Loss:2.303955	LR: 0.01
2019-06-05 11:00:55,920 - INFO: Iter: 170/64000	Loss:2.304528	LR: 0.01
2019-06-05 11:00:56,659 - INFO: Iter: 180/64000	Loss:2.301266	LR: 0.01
2019-06-05 11:00:57,406 - INFO: Iter: 190/64000	Loss:2.301184	LR: 0.01
2019-06-05 11:00:58,145 - INFO: Iter: 200/64000	Loss:2.304004	LR: 0.01
2019-06-05 11:00:58,869 - INFO: Iter: 210/64000	Loss:2.302418	LR: 0.01
2019-06-05 11:03:35,471 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.01, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 11:03:38,007 - INFO: Iter: 10/64000	Loss:2.305790	LR: 0.01
2019-06-05 11:03:38,805 - INFO: Iter: 20/64000	Loss:2.310958	LR: 0.01
2019-06-05 11:03:49,379 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.01, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 11:03:51,306 - INFO: Iter: 10/64000	Loss:2.301716	LR: 0.01
2019-06-05 11:03:51,668 - INFO: Iter: 20/64000	Loss:2.298239	LR: 0.01
2019-06-05 11:03:52,022 - INFO: Iter: 30/64000	Loss:2.298674	LR: 0.01
2019-06-05 11:03:52,393 - INFO: Iter: 40/64000	Loss:2.295554	LR: 0.01
2019-06-05 11:03:52,759 - INFO: Iter: 50/64000	Loss:2.307446	LR: 0.01
2019-06-05 11:03:53,112 - INFO: Iter: 60/64000	Loss:2.301131	LR: 0.01
2019-06-05 11:03:53,477 - INFO: Iter: 70/64000	Loss:2.287847	LR: 0.01
2019-06-05 11:03:53,854 - INFO: Iter: 80/64000	Loss:2.297149	LR: 0.01
2019-06-05 11:03:54,210 - INFO: Iter: 90/64000	Loss:2.284198	LR: 0.01
2019-06-05 11:03:54,560 - INFO: Iter: 100/64000	Loss:2.285909	LR: 0.01
2019-06-05 11:03:54,931 - INFO: Iter: 110/64000	Loss:2.275873	LR: 0.01
2019-06-05 11:03:55,291 - INFO: Iter: 120/64000	Loss:2.240531	LR: 0.01
2019-06-05 11:03:55,657 - INFO: Iter: 130/64000	Loss:2.268174	LR: 0.01
2019-06-05 11:03:56,014 - INFO: Iter: 140/64000	Loss:2.186503	LR: 0.01
2019-06-05 11:03:56,397 - INFO: Iter: 150/64000	Loss:2.133850	LR: 0.01
2019-06-05 11:03:56,770 - INFO: Iter: 160/64000	Loss:2.064028	LR: 0.01
2019-06-05 11:03:57,130 - INFO: Iter: 170/64000	Loss:2.155416	LR: 0.01
2019-06-05 11:03:57,486 - INFO: Iter: 180/64000	Loss:2.103850	LR: 0.01
2019-06-05 11:03:57,845 - INFO: Iter: 190/64000	Loss:2.034308	LR: 0.01
2019-06-05 11:03:58,193 - INFO: Iter: 200/64000	Loss:2.038063	LR: 0.01
2019-06-05 11:03:58,542 - INFO: Iter: 210/64000	Loss:2.193106	LR: 0.01
2019-06-05 11:03:58,916 - INFO: Iter: 220/64000	Loss:2.081116	LR: 0.01
2019-06-05 11:03:59,265 - INFO: Iter: 230/64000	Loss:2.155832	LR: 0.01
2019-06-05 11:03:59,632 - INFO: Iter: 240/64000	Loss:2.110750	LR: 0.01
2019-06-05 11:03:59,988 - INFO: Iter: 250/64000	Loss:2.111681	LR: 0.01
2019-06-05 11:04:00,354 - INFO: Iter: 260/64000	Loss:2.107805	LR: 0.01
2019-06-05 11:04:00,732 - INFO: Iter: 270/64000	Loss:2.189230	LR: 0.01
2019-06-05 11:04:01,091 - INFO: Iter: 280/64000	Loss:1.953052	LR: 0.01
2019-06-05 11:04:01,458 - INFO: Iter: 290/64000	Loss:2.111494	LR: 0.01
2019-06-05 11:04:01,824 - INFO: Iter: 300/64000	Loss:2.000892	LR: 0.01
2019-06-05 11:04:02,196 - INFO: Iter: 310/64000	Loss:2.036449	LR: 0.01
2019-06-05 11:04:02,571 - INFO: Iter: 320/64000	Loss:1.926758	LR: 0.01
2019-06-05 11:04:02,945 - INFO: Iter: 330/64000	Loss:2.030331	LR: 0.01
2019-06-05 11:04:03,307 - INFO: Iter: 340/64000	Loss:2.083539	LR: 0.01
2019-06-05 11:04:03,667 - INFO: Iter: 350/64000	Loss:2.036627	LR: 0.01
2019-06-05 11:04:04,039 - INFO: Iter: 360/64000	Loss:1.953836	LR: 0.01
2019-06-05 11:04:04,401 - INFO: Iter: 370/64000	Loss:2.061171	LR: 0.01
2019-06-05 11:04:04,769 - INFO: Iter: 380/64000	Loss:1.896938	LR: 0.01
2019-06-05 11:04:05,094 - INFO: Iter: 390/64000	Loss:1.996065	LR: 0.01
2019-06-05 11:04:05,539 - INFO: Iter: 400/64000	Loss:1.805041	LR: 0.01
2019-06-05 11:04:05,922 - INFO: Iter: 410/64000	Loss:1.960453	LR: 0.01
2019-06-05 11:04:06,283 - INFO: Iter: 420/64000	Loss:2.093848	LR: 0.01
2019-06-05 11:05:43,583 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.01, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 11:05:45,917 - INFO: Iter: 10/64000	Loss:2.314071	LR: 0.01
2019-06-05 11:05:46,646 - INFO: Iter: 20/64000	Loss:2.301063	LR: 0.01
2019-06-05 11:05:47,404 - INFO: Iter: 30/64000	Loss:2.299549	LR: 0.01
2019-06-05 11:05:48,146 - INFO: Iter: 40/64000	Loss:2.302626	LR: 0.01
2019-06-05 11:05:48,914 - INFO: Iter: 50/64000	Loss:2.304648	LR: 0.01
2019-06-05 11:05:49,644 - INFO: Iter: 60/64000	Loss:2.301012	LR: 0.01
2019-06-05 11:05:50,384 - INFO: Iter: 70/64000	Loss:2.302765	LR: 0.01
2019-06-05 11:05:51,125 - INFO: Iter: 80/64000	Loss:2.305802	LR: 0.01
2019-06-05 11:05:51,875 - INFO: Iter: 90/64000	Loss:2.302437	LR: 0.01
2019-06-05 11:05:52,618 - INFO: Iter: 100/64000	Loss:2.301614	LR: 0.01
2019-06-05 11:05:53,351 - INFO: Iter: 110/64000	Loss:2.301174	LR: 0.01
2019-06-05 11:05:54,115 - INFO: Iter: 120/64000	Loss:2.301628	LR: 0.01
2019-06-05 11:05:54,867 - INFO: Iter: 130/64000	Loss:2.303129	LR: 0.01
2019-06-05 11:05:55,625 - INFO: Iter: 140/64000	Loss:2.303618	LR: 0.01
2019-06-05 11:05:56,385 - INFO: Iter: 150/64000	Loss:2.304098	LR: 0.01
2019-06-05 11:05:57,143 - INFO: Iter: 160/64000	Loss:2.303480	LR: 0.01
2019-06-05 11:05:57,863 - INFO: Iter: 170/64000	Loss:2.302532	LR: 0.01
2019-06-05 11:05:58,645 - INFO: Iter: 180/64000	Loss:2.302866	LR: 0.01
2019-06-05 11:05:59,427 - INFO: Iter: 190/64000	Loss:2.304282	LR: 0.01
2019-06-05 11:06:00,212 - INFO: Iter: 200/64000	Loss:2.301191	LR: 0.01
2019-06-05 11:06:00,994 - INFO: Iter: 210/64000	Loss:2.305880	LR: 0.01
2019-06-05 11:06:01,767 - INFO: Iter: 220/64000	Loss:2.298894	LR: 0.01
2019-06-05 11:06:02,545 - INFO: Iter: 230/64000	Loss:2.300693	LR: 0.01
2019-06-05 11:06:03,332 - INFO: Iter: 240/64000	Loss:2.305762	LR: 0.01
2019-06-05 11:06:24,965 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.01, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 11:06:26,892 - INFO: Iter: 10/64000	Loss:2.301738	LR: 0.01
2019-06-05 11:06:27,243 - INFO: Iter: 20/64000	Loss:2.305912	LR: 0.01
2019-06-05 11:06:27,608 - INFO: Iter: 30/64000	Loss:2.302402	LR: 0.01
2019-06-05 11:06:27,985 - INFO: Iter: 40/64000	Loss:2.297489	LR: 0.01
2019-06-05 11:06:28,350 - INFO: Iter: 50/64000	Loss:2.290854	LR: 0.01
2019-06-05 11:06:28,703 - INFO: Iter: 60/64000	Loss:2.291631	LR: 0.01
2019-06-05 11:06:29,092 - INFO: Iter: 70/64000	Loss:2.272925	LR: 0.01
2019-06-05 11:06:29,458 - INFO: Iter: 80/64000	Loss:2.247223	LR: 0.01
2019-06-05 11:06:29,826 - INFO: Iter: 90/64000	Loss:2.232132	LR: 0.01
2019-06-05 11:06:30,187 - INFO: Iter: 100/64000	Loss:2.205514	LR: 0.01
2019-06-05 11:06:30,555 - INFO: Iter: 110/64000	Loss:2.119631	LR: 0.01
2019-06-05 11:06:30,925 - INFO: Iter: 120/64000	Loss:2.044346	LR: 0.01
2019-06-05 11:06:31,280 - INFO: Iter: 130/64000	Loss:2.204860	LR: 0.01
2019-06-05 11:06:31,630 - INFO: Iter: 140/64000	Loss:2.086961	LR: 0.01
2019-06-05 11:06:32,004 - INFO: Iter: 150/64000	Loss:2.085347	LR: 0.01
2019-06-05 11:06:32,366 - INFO: Iter: 160/64000	Loss:2.014318	LR: 0.01
2019-06-05 11:06:32,741 - INFO: Iter: 170/64000	Loss:1.992696	LR: 0.01
2019-06-05 11:06:33,101 - INFO: Iter: 180/64000	Loss:2.091418	LR: 0.01
2019-06-05 11:06:33,460 - INFO: Iter: 190/64000	Loss:2.066191	LR: 0.01
2019-06-05 11:06:33,823 - INFO: Iter: 200/64000	Loss:1.971140	LR: 0.01
2019-06-05 11:06:34,187 - INFO: Iter: 210/64000	Loss:1.870463	LR: 0.01
2019-06-05 11:06:34,550 - INFO: Iter: 220/64000	Loss:1.905744	LR: 0.01
2019-06-05 11:06:38,414 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.1, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 11:06:40,367 - INFO: Iter: 10/64000	Loss:2.295374	LR: 0.1
2019-06-05 11:06:40,746 - INFO: Iter: 20/64000	Loss:2.303664	LR: 0.1
2019-06-05 11:06:41,116 - INFO: Iter: 30/64000	Loss:2.161574	LR: 0.1
2019-06-05 11:06:41,485 - INFO: Iter: 40/64000	Loss:2.172900	LR: 0.1
2019-06-05 11:06:41,859 - INFO: Iter: 50/64000	Loss:2.120121	LR: 0.1
2019-06-05 11:06:42,230 - INFO: Iter: 60/64000	Loss:2.039953	LR: 0.1
2019-06-05 11:06:42,608 - INFO: Iter: 70/64000	Loss:2.003642	LR: 0.1
2019-06-05 11:06:42,988 - INFO: Iter: 80/64000	Loss:2.032348	LR: 0.1
2019-06-05 11:06:43,350 - INFO: Iter: 90/64000	Loss:1.989814	LR: 0.1
2019-06-05 11:06:43,715 - INFO: Iter: 100/64000	Loss:2.040564	LR: 0.1
2019-06-05 11:06:44,080 - INFO: Iter: 110/64000	Loss:1.926674	LR: 0.1
2019-06-05 11:06:44,464 - INFO: Iter: 120/64000	Loss:2.058628	LR: 0.1
2019-06-05 11:06:44,839 - INFO: Iter: 130/64000	Loss:1.792094	LR: 0.1
2019-06-05 11:06:45,204 - INFO: Iter: 140/64000	Loss:1.874330	LR: 0.1
2019-06-05 11:06:45,578 - INFO: Iter: 150/64000	Loss:1.850828	LR: 0.1
2019-06-05 11:06:45,963 - INFO: Iter: 160/64000	Loss:1.866602	LR: 0.1
2019-06-05 11:06:46,328 - INFO: Iter: 170/64000	Loss:1.936381	LR: 0.1
2019-06-05 11:06:46,704 - INFO: Iter: 180/64000	Loss:1.733387	LR: 0.1
2019-06-05 11:06:47,072 - INFO: Iter: 190/64000	Loss:1.852811	LR: 0.1
2019-06-05 11:06:47,448 - INFO: Iter: 200/64000	Loss:1.901058	LR: 0.1
2019-06-05 11:06:47,820 - INFO: Iter: 210/64000	Loss:1.744736	LR: 0.1
2019-06-05 11:06:48,181 - INFO: Iter: 220/64000	Loss:1.580444	LR: 0.1
2019-06-05 11:06:48,544 - INFO: Iter: 230/64000	Loss:1.672065	LR: 0.1
2019-06-05 11:06:48,900 - INFO: Iter: 240/64000	Loss:1.910541	LR: 0.1
2019-06-05 11:06:49,275 - INFO: Iter: 250/64000	Loss:1.698530	LR: 0.1
2019-06-05 11:06:49,638 - INFO: Iter: 260/64000	Loss:1.800246	LR: 0.1
2019-06-05 11:06:50,030 - INFO: Iter: 270/64000	Loss:1.675183	LR: 0.1
2019-06-05 11:06:50,396 - INFO: Iter: 280/64000	Loss:1.722079	LR: 0.1
2019-06-05 11:06:50,759 - INFO: Iter: 290/64000	Loss:1.765060	LR: 0.1
2019-06-05 11:06:51,139 - INFO: Iter: 300/64000	Loss:1.758500	LR: 0.1
2019-06-05 11:06:51,502 - INFO: Iter: 310/64000	Loss:1.928009	LR: 0.1
2019-06-05 11:06:51,861 - INFO: Iter: 320/64000	Loss:1.694007	LR: 0.1
2019-06-05 11:06:52,233 - INFO: Iter: 330/64000	Loss:1.784396	LR: 0.1
2019-06-05 11:06:52,588 - INFO: Iter: 340/64000	Loss:1.834146	LR: 0.1
2019-06-05 11:06:52,968 - INFO: Iter: 350/64000	Loss:1.633539	LR: 0.1
2019-06-05 11:06:53,344 - INFO: Iter: 360/64000	Loss:1.816782	LR: 0.1
2019-06-05 11:06:53,713 - INFO: Iter: 370/64000	Loss:1.686662	LR: 0.1
2019-06-05 11:06:54,096 - INFO: Iter: 380/64000	Loss:1.660289	LR: 0.1
2019-06-05 11:06:54,433 - INFO: Iter: 390/64000	Loss:1.604204	LR: 0.1
2019-06-05 11:06:54,908 - INFO: Iter: 400/64000	Loss:1.665047	LR: 0.1
2019-06-05 11:06:55,272 - INFO: Iter: 410/64000	Loss:1.789847	LR: 0.1
2019-06-05 11:06:55,620 - INFO: Iter: 420/64000	Loss:1.732430	LR: 0.1
2019-06-05 11:06:55,991 - INFO: Iter: 430/64000	Loss:1.693660	LR: 0.1
2019-06-05 11:06:56,354 - INFO: Iter: 440/64000	Loss:1.734246	LR: 0.1
2019-06-05 11:07:16,636 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.1, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 11:07:18,572 - INFO: Iter: 10/64000	Loss:2.311186	LR: 0.1
2019-06-05 11:07:18,932 - INFO: Iter: 20/64000	Loss:2.295050	LR: 0.1
2019-06-05 11:07:19,291 - INFO: Iter: 30/64000	Loss:2.286045	LR: 0.1
2019-06-05 11:07:19,665 - INFO: Iter: 40/64000	Loss:2.227098	LR: 0.1
2019-06-05 11:07:20,043 - INFO: Iter: 50/64000	Loss:2.156924	LR: 0.1
2019-06-05 11:07:20,404 - INFO: Iter: 60/64000	Loss:2.069089	LR: 0.1
2019-06-05 11:07:20,755 - INFO: Iter: 70/64000	Loss:2.071964	LR: 0.1
2019-06-05 11:07:21,129 - INFO: Iter: 80/64000	Loss:2.085096	LR: 0.1
2019-06-05 11:07:21,493 - INFO: Iter: 90/64000	Loss:1.946816	LR: 0.1
2019-06-05 11:07:21,863 - INFO: Iter: 100/64000	Loss:2.079283	LR: 0.1
2019-06-05 11:07:22,226 - INFO: Iter: 110/64000	Loss:2.050439	LR: 0.1
2019-06-05 11:07:22,598 - INFO: Iter: 120/64000	Loss:2.165068	LR: 0.1
2019-06-05 11:07:22,971 - INFO: Iter: 130/64000	Loss:2.130827	LR: 0.1
2019-06-05 11:07:23,330 - INFO: Iter: 140/64000	Loss:2.109336	LR: 0.1
2019-06-05 11:07:23,711 - INFO: Iter: 150/64000	Loss:1.984996	LR: 0.1
2019-06-05 11:07:24,079 - INFO: Iter: 160/64000	Loss:1.904639	LR: 0.1
2019-06-05 11:07:24,445 - INFO: Iter: 170/64000	Loss:1.845298	LR: 0.1
2019-06-05 11:07:24,814 - INFO: Iter: 180/64000	Loss:1.949460	LR: 0.1
2019-06-05 11:07:25,178 - INFO: Iter: 190/64000	Loss:1.929516	LR: 0.1
2019-06-05 11:07:36,026 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.1, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 11:07:37,952 - INFO: Iter: 10/64000	Loss:2.307175	LR: 0.1
2019-06-05 11:07:38,317 - INFO: Iter: 20/64000	Loss:2.298943	LR: 0.1
2019-06-05 11:07:38,684 - INFO: Iter: 30/64000	Loss:2.310713	LR: 0.1
2019-06-05 11:07:39,048 - INFO: Iter: 40/64000	Loss:2.309177	LR: 0.1
2019-06-05 11:07:39,403 - INFO: Iter: 50/64000	Loss:2.305305	LR: 0.1
2019-06-05 11:07:39,770 - INFO: Iter: 60/64000	Loss:2.301186	LR: 0.1
2019-06-05 11:07:40,141 - INFO: Iter: 70/64000	Loss:2.300117	LR: 0.1
2019-06-05 11:07:40,494 - INFO: Iter: 80/64000	Loss:2.285289	LR: 0.1
2019-06-05 11:07:40,850 - INFO: Iter: 90/64000	Loss:2.174543	LR: 0.1
2019-06-05 11:07:41,200 - INFO: Iter: 100/64000	Loss:2.156546	LR: 0.1
2019-06-05 11:07:41,562 - INFO: Iter: 110/64000	Loss:2.108556	LR: 0.1
2019-06-05 11:07:41,930 - INFO: Iter: 120/64000	Loss:2.156636	LR: 0.1
2019-06-05 11:07:42,289 - INFO: Iter: 130/64000	Loss:2.178068	LR: 0.1
2019-06-05 11:07:42,649 - INFO: Iter: 140/64000	Loss:2.080976	LR: 0.1
2019-06-05 11:07:43,015 - INFO: Iter: 150/64000	Loss:2.030798	LR: 0.1
2019-06-05 11:07:43,377 - INFO: Iter: 160/64000	Loss:2.035377	LR: 0.1
2019-06-05 11:07:43,742 - INFO: Iter: 170/64000	Loss:1.955816	LR: 0.1
2019-06-05 11:07:44,109 - INFO: Iter: 180/64000	Loss:2.035431	LR: 0.1
2019-06-05 11:07:44,474 - INFO: Iter: 190/64000	Loss:2.023644	LR: 0.1
2019-06-05 11:07:44,835 - INFO: Iter: 200/64000	Loss:1.989139	LR: 0.1
2019-06-05 11:07:45,207 - INFO: Iter: 210/64000	Loss:1.831086	LR: 0.1
2019-06-05 11:07:45,562 - INFO: Iter: 220/64000	Loss:1.964278	LR: 0.1
2019-06-05 11:07:45,914 - INFO: Iter: 230/64000	Loss:1.920661	LR: 0.1
2019-06-05 11:07:46,272 - INFO: Iter: 240/64000	Loss:1.945533	LR: 0.1
2019-06-05 11:07:46,647 - INFO: Iter: 250/64000	Loss:1.978395	LR: 0.1
2019-06-05 11:07:47,012 - INFO: Iter: 260/64000	Loss:1.968281	LR: 0.1
2019-06-05 11:08:37,791 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.1, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 11:08:40,003 - INFO: Iter: 10/64000	Loss:2.320800	LR: 0.1
2019-06-05 11:08:40,638 - INFO: Iter: 20/64000	Loss:2.309462	LR: 0.1
2019-06-05 11:08:41,242 - INFO: Iter: 30/64000	Loss:2.300765	LR: 0.1
2019-06-05 11:08:41,864 - INFO: Iter: 40/64000	Loss:2.303165	LR: 0.1
2019-06-05 11:08:42,504 - INFO: Iter: 50/64000	Loss:2.306669	LR: 0.1
2019-06-05 11:08:43,131 - INFO: Iter: 60/64000	Loss:2.301240	LR: 0.1
2019-06-05 11:08:43,754 - INFO: Iter: 70/64000	Loss:2.299419	LR: 0.1
2019-06-05 11:08:44,365 - INFO: Iter: 80/64000	Loss:2.300925	LR: 0.1
2019-06-05 11:08:44,990 - INFO: Iter: 90/64000	Loss:2.304872	LR: 0.1
2019-06-05 11:08:45,604 - INFO: Iter: 100/64000	Loss:2.302071	LR: 0.1
2019-06-05 11:08:46,242 - INFO: Iter: 110/64000	Loss:2.308159	LR: 0.1
2019-06-05 11:08:46,878 - INFO: Iter: 120/64000	Loss:2.309084	LR: 0.1
2019-06-05 11:08:47,491 - INFO: Iter: 130/64000	Loss:2.298111	LR: 0.1
2019-06-05 11:08:48,108 - INFO: Iter: 140/64000	Loss:2.307477	LR: 0.1
2019-06-05 11:08:48,731 - INFO: Iter: 150/64000	Loss:2.309693	LR: 0.1
2019-06-05 11:08:49,357 - INFO: Iter: 160/64000	Loss:2.308512	LR: 0.1
2019-06-05 11:08:49,989 - INFO: Iter: 170/64000	Loss:2.301999	LR: 0.1
2019-06-05 11:08:58,204 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.1, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 11:09:00,353 - INFO: Iter: 10/64000	Loss:2.304682	LR: 0.1
2019-06-05 11:09:00,913 - INFO: Iter: 20/64000	Loss:2.302385	LR: 0.1
2019-06-05 11:09:01,461 - INFO: Iter: 30/64000	Loss:2.307281	LR: 0.1
2019-06-05 11:09:02,025 - INFO: Iter: 40/64000	Loss:2.304314	LR: 0.1
2019-06-05 11:09:02,571 - INFO: Iter: 50/64000	Loss:2.305470	LR: 0.1
2019-06-05 11:09:03,139 - INFO: Iter: 60/64000	Loss:2.311268	LR: 0.1
2019-06-05 11:09:03,688 - INFO: Iter: 70/64000	Loss:2.309657	LR: 0.1
2019-06-05 11:09:04,242 - INFO: Iter: 80/64000	Loss:2.294910	LR: 0.1
2019-06-05 11:09:04,800 - INFO: Iter: 90/64000	Loss:2.303039	LR: 0.1
2019-06-05 11:09:05,361 - INFO: Iter: 100/64000	Loss:2.298652	LR: 0.1
2019-06-05 11:09:05,914 - INFO: Iter: 110/64000	Loss:2.233219	LR: 0.1
2019-06-05 11:09:06,474 - INFO: Iter: 120/64000	Loss:2.269180	LR: 0.1
2019-06-05 11:09:07,022 - INFO: Iter: 130/64000	Loss:2.163808	LR: 0.1
2019-06-05 11:09:07,584 - INFO: Iter: 140/64000	Loss:2.011647	LR: 0.1
2019-06-05 11:09:08,138 - INFO: Iter: 150/64000	Loss:2.143543	LR: 0.1
2019-06-05 11:09:08,676 - INFO: Iter: 160/64000	Loss:2.015405	LR: 0.1
2019-06-05 11:09:09,222 - INFO: Iter: 170/64000	Loss:2.120411	LR: 0.1
2019-06-05 11:09:09,765 - INFO: Iter: 180/64000	Loss:2.121691	LR: 0.1
2019-06-05 11:09:10,312 - INFO: Iter: 190/64000	Loss:2.012308	LR: 0.1
2019-06-05 11:09:10,859 - INFO: Iter: 200/64000	Loss:2.101590	LR: 0.1
2019-06-05 11:09:11,413 - INFO: Iter: 210/64000	Loss:2.039614	LR: 0.1
2019-06-05 11:09:11,967 - INFO: Iter: 220/64000	Loss:1.926171	LR: 0.1
2019-06-05 11:09:12,512 - INFO: Iter: 230/64000	Loss:1.971776	LR: 0.1
2019-06-05 11:09:13,070 - INFO: Iter: 240/64000	Loss:1.986182	LR: 0.1
2019-06-05 11:09:13,616 - INFO: Iter: 250/64000	Loss:2.033709	LR: 0.1
2019-06-05 11:09:14,169 - INFO: Iter: 260/64000	Loss:2.062424	LR: 0.1
2019-06-05 11:09:14,732 - INFO: Iter: 270/64000	Loss:1.934210	LR: 0.1
2019-06-05 11:09:15,279 - INFO: Iter: 280/64000	Loss:2.026806	LR: 0.1
2019-06-05 11:09:15,824 - INFO: Iter: 290/64000	Loss:1.966921	LR: 0.1
2019-06-05 11:09:24,645 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.01, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 11:09:26,837 - INFO: Iter: 10/64000	Loss:2.295992	LR: 0.01
2019-06-05 11:09:27,423 - INFO: Iter: 20/64000	Loss:2.308440	LR: 0.01
2019-06-05 11:09:28,020 - INFO: Iter: 30/64000	Loss:2.301368	LR: 0.01
2019-06-05 11:09:28,632 - INFO: Iter: 40/64000	Loss:2.302849	LR: 0.01
2019-06-05 11:09:29,220 - INFO: Iter: 50/64000	Loss:2.297438	LR: 0.01
2019-06-05 11:09:29,822 - INFO: Iter: 60/64000	Loss:2.303239	LR: 0.01
2019-06-05 11:09:30,429 - INFO: Iter: 70/64000	Loss:2.300680	LR: 0.01
2019-06-05 11:09:31,030 - INFO: Iter: 80/64000	Loss:2.302445	LR: 0.01
2019-06-05 11:09:31,620 - INFO: Iter: 90/64000	Loss:2.302628	LR: 0.01
2019-06-05 11:09:32,218 - INFO: Iter: 100/64000	Loss:2.305099	LR: 0.01
2019-06-05 11:09:32,821 - INFO: Iter: 110/64000	Loss:2.304607	LR: 0.01
2019-06-05 11:09:33,424 - INFO: Iter: 120/64000	Loss:2.304578	LR: 0.01
2019-06-05 11:09:34,025 - INFO: Iter: 130/64000	Loss:2.303465	LR: 0.01
2019-06-05 11:09:34,631 - INFO: Iter: 140/64000	Loss:2.304046	LR: 0.01
2019-06-05 11:09:35,235 - INFO: Iter: 150/64000	Loss:2.305476	LR: 0.01
2019-06-05 11:09:35,826 - INFO: Iter: 160/64000	Loss:2.302880	LR: 0.01
2019-06-05 11:09:36,427 - INFO: Iter: 170/64000	Loss:2.301928	LR: 0.01
2019-06-05 11:09:37,081 - INFO: Iter: 180/64000	Loss:2.303497	LR: 0.01
2019-06-05 11:09:37,704 - INFO: Iter: 190/64000	Loss:2.303591	LR: 0.01
2019-06-05 11:09:38,325 - INFO: Iter: 200/64000	Loss:2.302757	LR: 0.01
2019-06-05 11:09:38,987 - INFO: Iter: 210/64000	Loss:2.304748	LR: 0.01
2019-06-05 11:09:39,633 - INFO: Iter: 220/64000	Loss:2.304628	LR: 0.01
2019-06-05 11:09:40,244 - INFO: Iter: 230/64000	Loss:2.303915	LR: 0.01
2019-06-05 11:09:40,875 - INFO: Iter: 240/64000	Loss:2.300977	LR: 0.01
2019-06-05 11:09:41,503 - INFO: Iter: 250/64000	Loss:2.303892	LR: 0.01
2019-06-05 11:09:42,099 - INFO: Iter: 260/64000	Loss:2.301408	LR: 0.01
2019-06-05 11:09:42,722 - INFO: Iter: 270/64000	Loss:2.299949	LR: 0.01
2019-06-05 11:09:43,329 - INFO: Iter: 280/64000	Loss:2.301792	LR: 0.01
2019-06-05 11:09:43,934 - INFO: Iter: 290/64000	Loss:2.305462	LR: 0.01
2019-06-05 11:09:44,561 - INFO: Iter: 300/64000	Loss:2.301980	LR: 0.01
2019-06-05 11:09:45,210 - INFO: Iter: 310/64000	Loss:2.302838	LR: 0.01
2019-06-05 11:09:45,885 - INFO: Iter: 320/64000	Loss:2.302603	LR: 0.01
2019-06-05 11:09:49,154 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.001, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-05 11:09:51,359 - INFO: Iter: 10/64000	Loss:2.301148	LR: 0.001
2019-06-05 11:09:51,974 - INFO: Iter: 20/64000	Loss:2.301388	LR: 0.001
2019-06-05 11:09:52,587 - INFO: Iter: 30/64000	Loss:2.318139	LR: 0.001
2019-06-05 11:09:53,219 - INFO: Iter: 40/64000	Loss:2.304879	LR: 0.001
2019-06-05 11:09:53,814 - INFO: Iter: 50/64000	Loss:2.310514	LR: 0.001
2019-06-05 11:09:54,442 - INFO: Iter: 60/64000	Loss:2.294750	LR: 0.001
2019-06-05 11:09:55,046 - INFO: Iter: 70/64000	Loss:2.307260	LR: 0.001
2019-06-05 11:09:55,662 - INFO: Iter: 80/64000	Loss:2.302565	LR: 0.001
2019-06-05 11:09:56,282 - INFO: Iter: 90/64000	Loss:2.312118	LR: 0.001
2019-06-05 11:09:56,892 - INFO: Iter: 100/64000	Loss:2.320319	LR: 0.001
2019-06-05 11:09:57,508 - INFO: Iter: 110/64000	Loss:2.310137	LR: 0.001
2019-06-05 11:09:58,129 - INFO: Iter: 120/64000	Loss:2.295601	LR: 0.001
2019-06-05 11:09:58,738 - INFO: Iter: 130/64000	Loss:2.303459	LR: 0.001
2019-06-05 11:09:59,357 - INFO: Iter: 140/64000	Loss:2.296075	LR: 0.001
2019-06-05 11:09:59,980 - INFO: Iter: 150/64000	Loss:2.306643	LR: 0.001
2019-06-05 11:10:00,594 - INFO: Iter: 160/64000	Loss:2.303970	LR: 0.001
2019-06-05 11:10:01,227 - INFO: Iter: 170/64000	Loss:2.310363	LR: 0.001
2019-06-05 11:10:01,837 - INFO: Iter: 180/64000	Loss:2.297713	LR: 0.001
2019-06-05 11:10:02,446 - INFO: Iter: 190/64000	Loss:2.315109	LR: 0.001
2019-06-06 10:02:01,738 - INFO: Namespace(adam=False, batch_size=128, log_path='./train.log', lr=0.01, m=0.9, n_iter=64000, no_cuda=False, save=False, save_path='./result', val=False, val_interval=1000)
2019-06-06 10:02:06,586 - INFO: Iter: 10/64000	Loss:2.309128	LR: 0.01
2019-06-06 10:02:07,684 - INFO: Iter: 20/64000	Loss:2.288453	LR: 0.01
2019-06-06 10:02:08,708 - INFO: Iter: 30/64000	Loss:2.284240	LR: 0.01
2019-06-06 10:02:09,687 - INFO: Iter: 40/64000	Loss:2.231426	LR: 0.01
2019-06-06 10:02:10,678 - INFO: Iter: 50/64000	Loss:2.163605	LR: 0.01
2019-06-06 10:02:11,673 - INFO: Iter: 60/64000	Loss:2.092957	LR: 0.01
2019-06-06 10:02:12,654 - INFO: Iter: 70/64000	Loss:1.967422	LR: 0.01
2019-06-06 10:02:13,719 - INFO: Iter: 80/64000	Loss:1.923520	LR: 0.01
2019-06-06 10:02:14,815 - INFO: Iter: 90/64000	Loss:1.927611	LR: 0.01
2019-06-06 10:02:15,929 - INFO: Iter: 100/64000	Loss:1.863329	LR: 0.01
2019-06-06 10:02:17,060 - INFO: Iter: 110/64000	Loss:1.865647	LR: 0.01
2019-06-06 10:02:18,163 - INFO: Iter: 120/64000	Loss:1.867275	LR: 0.01
2019-06-06 10:02:19,334 - INFO: Iter: 130/64000	Loss:1.744799	LR: 0.01
2019-06-06 10:02:20,431 - INFO: Iter: 140/64000	Loss:1.868679	LR: 0.01
2019-06-06 10:02:21,478 - INFO: Iter: 150/64000	Loss:1.949808	LR: 0.01
2019-06-06 10:02:22,494 - INFO: Iter: 160/64000	Loss:1.845969	LR: 0.01
2019-06-06 10:02:23,471 - INFO: Iter: 170/64000	Loss:1.803269	LR: 0.01
2019-06-06 10:02:24,484 - INFO: Iter: 180/64000	Loss:1.759776	LR: 0.01
2019-06-06 10:02:25,481 - INFO: Iter: 190/64000	Loss:1.766330	LR: 0.01
2019-06-06 10:02:26,466 - INFO: Iter: 200/64000	Loss:1.704573	LR: 0.01
2019-06-06 10:02:27,499 - INFO: Iter: 210/64000	Loss:1.753405	LR: 0.01
2019-06-06 10:02:28,484 - INFO: Iter: 220/64000	Loss:1.658076	LR: 0.01
2019-06-06 10:02:29,506 - INFO: Iter: 230/64000	Loss:1.761036	LR: 0.01
2019-06-06 10:02:30,491 - INFO: Iter: 240/64000	Loss:1.619193	LR: 0.01
2019-06-06 10:02:31,574 - INFO: Iter: 250/64000	Loss:1.518404	LR: 0.01
2019-06-06 10:02:32,631 - INFO: Iter: 260/64000	Loss:1.527132	LR: 0.01
2019-06-06 10:02:33,746 - INFO: Iter: 270/64000	Loss:1.647531	LR: 0.01
